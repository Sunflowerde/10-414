# Question 2: Loading MNIST data
## 1.理解 data 中的文件
* data 中有 4 个文件，分别表示训练集和测试集
* 以 train-images-idx3-ubyte.gz 为例，train 表示其为训练集，images 表示其为图像数据，idx 指二进制名，3 表示是 3 维数据，ubyte 是 无符号字节，从 0-255，存储灰度图像，.gz 是后缀名
* t10k 则表示为测试集(test)，共有 10000 个数据
## 2.具体的文件格式
### 通用规则
* 一，解压缩，用到了 `gzip` 库
* 二，文件内容是二进制数据，需要用 `rb` 模式读取
* 三，存储的整数为大端字节数，与正常的计数相同，最高位在最左侧
### 标签文件的结构
* 偏移量 0-3，0x00000801，08 表示数据类型为 unsigned byte，01 代表数据是一维的
* 偏移量 4-7，标签的总数量（如 60000 个）
* 偏移量 8-，连续的标签数据（共有 60000 个字节）
### 图像文件的结构
* 偏移量 0-3，0x00000803，03 代表数据是三维的
* 偏移量 4-7，图片的总数量
* 偏移量 8-11，图片的高度（对 MNIST 是 28）
* 偏移量 12-15，图像的宽度（28）
* 偏移量 16-，连续的像素数据，每个像素占 1 byte，数据优先按行排列
## 3.Python 处理思路
* 1.解压并打开文件，使用 `gzip.open(filename, "rb")`
* 2.读取字节：使用 `.read(num_bytes)` 读取指定数量的字节，使用 `int.from_bytes(bytes, "big")` 将读取到的字节串转换为整数，并采用大端序
* 3.读取数据
* 4.处理数据，对于图像数据，需要将其 `reshape` 成 （图片数量，784），需要将数据类型从 `uint8` 转为 `float32`；对于标签数据，直接保留即可
# Question 3: Softmax Loss
* 为了加快收敛速度，我们采用 SGD，所以第一个维度为 batch_size
* 对于 Z 的第二个维度，是对所有标签的可能的预测值，还未经 softmax 处理
* 对于 y，表示 batch_size 里每个图片的真实标签
* 对于每一个行向量，需要将所有 exp 相加，取 log，再减去所对应的标签索引的值
* 为了 vectorization，需要将 Z_y 单独成一个列向量
* 所以需要用到高级索引
* 语法为 `array[rows, cols]`，先要用 `np.arange(batch_size)` 生成所有行索引，再用 y 直接当作列索引
```python
row_index = np.arange(batch_size)
z_y = Z[row_index, y]
```
* 再考虑处理 Z
* 我们先需要对所有元素取 exp，然后再对行求和，取 log
* 需要注意维度问题，由于 Z_y 是 (batch_size,)，我们不需要 keepdims，否则会出现行向量与列向量的广播，迅速耗光内存
* 最后作差再取平均值即可得到最终答案